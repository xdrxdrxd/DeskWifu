# core/pet_logic.py
import time
import logging
import random
import uuid
import re
import json
import threading
from typing import Dict, Any, Optional, List
from datetime import datetime, time as dt_time

import google.generativeai as genai
import config
from database import DatabaseManager
from services.base_services import LLMService, SearchService
from services.llm_service import GeminiService
from core.emotion_system import EmotionSystem
from core.personality_system import PersonalitySystem
from core.memory_system import MemorySystem
from tkinter import messagebox

class PetLogic:
    """æ‡‰ç”¨ç¨‹å¼çš„æ ¸å¿ƒæ¥­å‹™é‚è¼¯ï¼Œä¸å«UIã€‚"""

    def __init__(self, db_manager: DatabaseManager, llm_service: Optional[LLMService], search_service: Optional[SearchService]):
        self.db = db_manager
        self.llm = llm_service
        self.search = search_service
        
        self.settings = self._load_all_settings()
        self.user_id = self.settings.get(config.SETTING_USER_ID)
        if not self.user_id or self.user_id == "default_user":
            self.user_id = str(uuid.uuid4())
            self.db.save_app_setting(config.SETTING_USER_ID, self.user_id)
            self.settings[config.SETTING_USER_ID] = self.user_id
        
        self.personality_system = PersonalitySystem(self.db, self.user_id, self.settings, self.llm)
        self.emotion_system = EmotionSystem(self.db, self.user_id, self.settings, self.llm, self.personality_system)
        self.memory_system = MemorySystem(self.db, self.llm, self.user_id, self.settings)
        
        self.llm_history: List[Dict[str, Any]] = []
        self.last_interaction_time = time.time()
        self.last_proactive_chat_time = time.time()
        self.is_sleeping = False
        self.is_processing_llm = False
        self.last_pet_spoken_response: Optional[str] = None
        self.last_pet_internal_thought: Optional[str] = None
        self.last_user_input_leading_to_response: Optional[str] = None

        search_tool_func = genai.types.FunctionDeclaration(
            name="custom_search",
            description="ç•¶ä½ éœ€è¦æŸ¥è©¢æœ€æ–°çš„ã€çœŸå¯¦ä¸–ç•Œçš„è³‡è¨Šï¼ˆä¾‹å¦‚æ–°èã€å¤©æ°£ã€ç‰¹å®šä¸»é¡Œçš„è³‡æ–™ï¼‰ï¼Œæˆ–è€…ä½ ä¸ç¢ºå®šçš„çŸ¥è­˜æ™‚ï¼Œä½¿ç”¨é€™å€‹å·¥å…·é€²è¡Œç¶²è·¯æœå°‹ã€‚",
            parameters={
                "type_": "OBJECT",
                "properties": {"query": {"type_": "STRING", "description": "ä½ æƒ³æœå°‹çš„é—œéµå­—è©ã€‚"}},
                "required": ["query"]
            },
        )
        self.tool_kit = [genai.types.Tool(function_declarations=[search_tool_func])]
        self.available_tools = {
            "custom_search": self.search.search if self.search else lambda query: {"error": "Search service is not enabled."}
        }
        logging.info("PetLogic native tools have been defined.")

    def reinitialize_llm_service(self) -> bool:
        """å˜—è©¦ä½¿ç”¨è³‡æ–™åº«ä¸­å„²å­˜çš„ API é‡‘é‘°é‡æ–°åˆå§‹åŒ– LLM æœå‹™ã€‚"""
        logging.info("Attempting to re-initialize LLM service...")
        api_key = self.db.get_api_key('gemini_api_key')
        if not api_key:
            logging.error("Re-initialization failed: API key not found in database.")
            return False
        try:
            model_name = self.db.load_app_setting(
                config.SETTING_SELECTED_LLM,
                config.DEFAULT_APP_SETTINGS[config.SETTING_SELECTED_LLM]
            )
            self.llm = GeminiService(api_key=api_key, model_name=model_name)
            self.memory_system.llm = self.llm
            self.personality_system.llm = self.llm
            logging.info("LLM service re-initialized successfully.")
            return True
        except Exception as e:
            logging.error(f"Failed to re-initialize GeminiService: {e}", exc_info=True)
            self.llm = None
            self.memory_system.llm = None
            self.personality_system.llm = None
            return False

    def _load_all_settings(self) -> Dict[str, Any]:
        """å¾è³‡æ–™åº«è¼‰å…¥æ‰€æœ‰æ‡‰ç”¨ç¨‹å¼è¨­å®š"""
        settings = {}
        for key, default_value in config.DEFAULT_APP_SETTINGS.items():
            settings[key] = self.db.load_app_setting(key, default_value)
        logging.info("All application settings loaded into PetLogic.")
        return settings

    def get_initial_state(self) -> Dict[str, Any]:
        """ç²å–å¯µç‰©å•Ÿå‹•æ™‚çš„åˆå§‹ç‹€æ…‹ï¼Œä¾›UIä½¿ç”¨"""
        self._check_sleep_schedule()
        dominant_emotion = "sleepy" if self.is_sleeping else self.emotion_system.get_dominant_emotion_for_display()
        return {
            "dominant_emotion": dominant_emotion,
            "initial_message": "å“ˆå›‰ï¼ä»Šå¤©æƒ³èŠäº›ä»€éº¼å‘€ï¼Ÿ",
            "is_sleeping": self.is_sleeping,
            "llm_ready": self.llm is not None
        }

    def handle_user_input(self, user_text: str) -> Dict[str, Any]:
        """è™•ç†ä½¿ç”¨è€…è¼¸å…¥çš„ä¸»æµç¨‹ï¼Œç¾åœ¨åŒ…å«å·¥å…·å‘¼å«å¾ªç’°å’Œå°è©±å¾Œå­¸ç¿’ã€‚"""
        logging.info(f"--- é–‹å§‹è™•ç†ä½¿ç”¨è€…è¼¸å…¥ --- : '{user_text}'")
        self.is_processing_llm = True
        try:
            if self.is_sleeping:
                return {"display_text": "zzz... (å°æ˜Ÿé‚„åœ¨ç¡è¦º)", "new_emotion_for_ui": "sleepy", "tag": "system"}
            if not self.llm:
                return {"display_text": "å—š...æˆ‘çš„å¤§è…¦å¥½åƒç½·å·¥äº†ï¼Œè«‹æª¢æŸ¥APIé‡‘é‘°è¨­å®šå–”ã€‚", "new_emotion_for_ui": "sad", "tag": "pet"}

            self.last_interaction_time = time.time()
            self.last_user_input_leading_to_response = user_text
            
            sensed_emotions = self.llm.analyze_text_for_emotions(user_text) if self.llm else {}
            self.memory_system.save_memory(
                content=f"ä½¿ç”¨è€…èªª: {user_text}", importance=2,
                pet_emotions=self.emotion_system.get_current_emotions(), user_emotions=sensed_emotions
            )
            prompt_details = self._build_llm_prompt(user_text, sensed_emotions, request_type="user_submit")
            llm_result = self.llm.generate_content(prompt_details)
            
            if llm_result.get("tool_call_request"):
                tool_request = llm_result["tool_call_request"]
                tool_name = tool_request["name"]
                tool_args = tool_request["args"]
                if tool_name in self.available_tools:
                    logging.info(f"Executing tool '{tool_name}' with args: {tool_args}")
                    tool_function = self.available_tools[tool_name]
                    tool_result = tool_function(**tool_args)
                else:
                    logging.error(f"LLM called an unknown tool: {tool_name}")
                    tool_result = {"error": f"Tool '{tool_name}' not found."}

                logging.info(f"Tool '{tool_name}' executed. Result: {str(tool_result)[:200]}...")
                messages_for_next_turn = llm_result["messages_history_for_next_turn"]
                messages_for_next_turn.append({
                    "role": "tool",
                    "parts": [{"function_response": {"name": tool_name, "response": tool_result}}]
                })
                
                logging.debug("LLMService: Second call to Gemini model with tool results.")
                final_response = self.llm.model.generate_content(
                    messages_for_next_turn,
                    generation_config=prompt_details["generation_config"]
                )
                raw_llm_output_text = final_response.text.strip()
                parsed_output = self.llm._parse_structured_output(raw_llm_output_text)
                spoken_response = parsed_output.get("spoken_response", raw_llm_output_text)
                internal_thought = parsed_output.get("internal_thought", "(ä½¿ç”¨å·¥å…·å¾Œé€²è¡Œç¸½çµ)")
            else:
                spoken_response = llm_result.get("spoken_response", "æˆ‘...å¥½åƒä¸çŸ¥é“è©²èªªä»€éº¼äº†ã€‚")
                internal_thought = llm_result.get("internal_thought")

            self.last_pet_spoken_response = spoken_response
            self.last_pet_internal_thought = internal_thought
            if internal_thought:
                self.memory_system.save_memory(
                    content=f"å°æ˜Ÿæ€è€ƒ: {internal_thought}", importance=0,
                    pet_emotions=self.emotion_system.get_current_emotions()
                )
            self.llm_history.append({"role": "user", "parts": [{"text": user_text}]})
            self.llm_history.append({"role": "model", "parts": [{"text": spoken_response}]})
            self.memory_system.save_memory(
                content=f"å°æ˜Ÿèªª: {spoken_response}", importance=1,
                pet_emotions=self.emotion_system.get_current_emotions()
            )
            self.personality_system.learn_from_user_text_async(user_text)
            self.personality_system.learn_from_pet_text_async(spoken_response)

            return {
                "display_text": spoken_response,
                "internal_thought": internal_thought,
                "new_emotion_for_ui": self.emotion_system.get_dominant_emotion_for_display(),
                "tag": "pet"
            }
        finally:
            self.is_processing_llm = False

    def _build_llm_prompt(self, user_message: str, user_sensed_emotions: Dict, request_type: str) -> Dict[str, Any]:
        """å»ºæ§‹å®Œæ•´çš„ LLM æç¤º"""
        request_structured_output = request_type in ["user_submit", "proactive_chat", "self_talk"]
        output_format_instruction = (
            "\n\né‡è¦è¼¸å‡ºæ ¼å¼æŒ‡ç¤ºï¼š\n"
            "è«‹åš´æ ¼ä»¥å–®ä¸€çš„ JSON ç‰©ä»¶æ ¼å¼è¼¸å‡ºä½ çš„å®Œæ•´å›æ‡‰ã€‚æ­¤ JSON ç‰©ä»¶å¿…é ˆåŒ…å«ä»¥ä¸‹å…©å€‹éµï¼š\n"
            "1. `internal_thought`: (å­—ä¸²) ä»£è¡¨ä½ åœ¨èªªè©±å‰çš„å…§å¿ƒæ€è€ƒã€æ„Ÿå—ã€æ„åœ–æˆ–ç°¡è¦è¨ˆç•«ã€‚\n"
            "2. `spoken_response`: (å­—ä¸²) ä»£è¡¨ä½ æœ€çµ‚æ±ºå®šå°ä½¿ç”¨è€…èªªå‡ºçš„è©±ã€‚\n"
            "```json\n"
            "{\n"
            "  \"internal_thought\": \"ä½¿ç”¨è€…è½èµ·ä¾†å¾ˆé–‹å¿ƒï¼Œæˆ‘ä¹Ÿè·Ÿè‘—é–‹å¿ƒèµ·ä¾†äº†ï¼\",\n"
            "  \"spoken_response\": \"è½èµ·ä¾†æ˜¯ä»¶å¾ˆæ£’çš„äº‹è€¶ï¼æˆ‘ä¹Ÿç‚ºä½ æ„Ÿåˆ°é«˜èˆˆï¼\"\n"
            "}\n"
            "```"
        ) if request_structured_output else "\n\nè«‹ç›´æ¥è¼¸å‡ºä½ èªç‚ºåˆé©çš„å›æ‡‰æ–‡å­—ã€‚"

        system_prompt_parts: List[str] = [
            config.CHARACTER_PROFILE,
            "ä½ æ“æœ‰[æœå°‹]ç­‰å·¥å…·ï¼Œå¯ä»¥åœ¨éœ€è¦æ™‚å‘¼å«å®ƒå€‘ã€‚",
            self.personality_system.get_personality_description(),
            self.personality_system.get_demographic_description(),
            self.personality_system.get_attachment_description_for_llm(),
            self.personality_system.get_self_efficacy_description_for_llm(),
            self.personality_system.get_neuro_state_description_for_llm(), # --- [æ–°å¢] é€™ä¸€è¡Œ ---
            self.personality_system.get_characteristics_description_for_llm(),
            f"ç¾åœ¨æ˜¯ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}ã€‚"
        ]
        
        memories = self.memory_system.get_memories_for_prompt()
        if memories.get("stm"):
            formatted_stms = ["\nä»¥ä¸‹æ˜¯ä½ æœ€è¿‘çš„ä¸€äº›é‡è¦å°è©±ç‰‡æ®µï¼š"] + [f"- {(time.time() - mem['timestamp']) / 60:.0f}åˆ†é˜å‰: {mem['content']}" for mem in memories["stm"]]
            system_prompt_parts.append("\n".join(formatted_stms))
        if memories.get("ltm"):
            formatted_ltms = ["\nä»¥ä¸‹æ˜¯ä½ çš„ä¸€äº›é•·æœŸè¨˜æ†¶æ‘˜è¦ï¼š"] + [f"- æˆ‘è¨˜å¾—ï¼šã€{mem['content']}ã€" for mem in memories["ltm"]]
            system_prompt_parts.append("\n".join(formatted_ltms))
        
        system_prompt_parts.append(output_format_instruction)
        final_system_prompt = "\n\n".join(filter(None, system_prompt_parts))

        messages_for_llm: List[Dict[str, Any]] = [
            {"role": "user", "parts": [{"text": final_system_prompt}]},
            {"role": "model", "parts": [{"text": "æˆ‘æ˜ç™½äº†ã€‚æˆ‘æœƒåŸºæ–¼ä»¥ä¸Šæ‰€æœ‰è³‡è¨Šï¼Œæ‰®æ¼”å¥½ã€Œå°æ˜Ÿã€é€™å€‹è§’è‰²ï¼Œä¸¦åš´æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼ä¾†å›æ‡‰ã€‚"}]}
        ]
        messages_for_llm.extend(self.llm_history[-10:])
        messages_for_llm.append({"role": "user", "parts": [{"text": user_message}]})
        
        generation_config = {
            "temperature": self.settings.get(config.SETTING_LLM_TEMP, 0.75),
            "max_output_tokens": self.settings.get(config.SETTING_LLM_MAX_TOKENS, 700)
        }
        
        return {
            "contents": messages_for_llm,
            "generation_config": generation_config,
            "expect_structured_output": request_structured_output,
            "tools": self.tool_kit if self.search and self.search.is_enabled else None
        }

    def _check_sleep_schedule(self):
        """æª¢æŸ¥ä¸¦æ›´æ–°å¯µç‰©çš„ç¡çœ ç‹€æ…‹"""
        now = datetime.now()
        current_time_val = now.time()
        bedtime_h = int(self.settings.get(config.SETTING_BEDTIME_HOUR, 1))
        bedtime_m = int(self.settings.get(config.SETTING_BEDTIME_MINUTE, 0))
        wakeup_h = int(self.settings.get(config.SETTING_WAKEUP_HOUR, 7))
        wakeup_m = int(self.settings.get(config.SETTING_WAKEUP_MINUTE, 0))
        print(f"DEBUG SLEEP CHECK: Reading bedtime as {bedtime_h}:{bedtime_m}, wakeup as {wakeup_h}:{wakeup_m}")
        bed_time = dt_time(bedtime_h, bedtime_m)
        wake_time = dt_time(wakeup_h, wakeup_m)
        was_sleeping = self.is_sleeping
        if bed_time <= wake_time:
            self.is_sleeping = bed_time <= current_time_val < wake_time
        else:
            self.is_sleeping = current_time_val >= bed_time or current_time_val < wake_time
        if self.is_sleeping and not was_sleeping:
            self.memory_system.save_memory("å°æ˜Ÿé€²å…¥ç¡çœ ç‹€æ…‹ã€‚", 0, self.emotion_system.get_current_emotions())
        elif not self.is_sleeping and was_sleeping:
            self.memory_system.save_memory("å°æ˜Ÿç¡é†’äº†ã€‚", 0, self.emotion_system.get_current_emotions())

    def check_for_proactive_action(self) -> Optional[Dict[str, Any]]:
        """æª¢æŸ¥æ˜¯å¦æ‡‰åŸ·è¡Œä¸»å‹•è¡Œç‚º"""
        if self.is_processing_llm or self.is_sleeping or not self.llm: return None
        now = time.time()
        min_interval, max_interval = (120, 300) # ç°¡åŒ–
        if now - self.last_proactive_chat_time < random.uniform(min_interval, max_interval): return None
        if now - self.last_interaction_time < 60: return None
        logging.info("Proactive chat conditions met. Initiating.")
        self.last_proactive_chat_time = now
        prompt_theme = "ä½ ç¾åœ¨æƒ³è¦ä¸»å‹•è·Ÿä½¿ç”¨è€…èªªäº›è©±ã€‚è«‹è‡ªç„¶åœ°ã€ç°¡çŸ­åœ°èªªå¹¾å¥è©±ã€‚"
        self.is_processing_llm = True
        try:
            prompt_details = self._build_llm_prompt(prompt_theme, {}, "proactive_chat")
            llm_result = self.llm.generate_content(prompt_details)
            spoken_response = llm_result.get("spoken_response")
            if spoken_response:
                self.memory_system.save_memory(f"å°æ˜Ÿ(ä¸»å‹•): {spoken_response}", 1, self.emotion_system.get_current_emotions())
                self.llm_history.append({"role": "model", "parts": [{"text": spoken_response}]})
                return {"display_text": spoken_response, "new_emotion_for_ui": self.emotion_system.get_dominant_emotion_for_display(), "tag": "pet"}
            return None
        finally:
            self.is_processing_llm = False
            
    def periodic_maintenance(self):
        """åŸ·è¡Œæ‰€æœ‰å®šæœŸçš„èƒŒæ™¯ç¶­è­·ä»»å‹™ã€‚"""
        logging.debug("Performing periodic maintenance tasks...")
        self._check_sleep_schedule()

        # --- [æ–°å¢] å…ˆåŸ·è¡Œ personality_system ç¶­è­·ï¼Œä»¥æ›´æ–° effective åƒæ•¸ ---
        self.personality_system.periodic_maintenance() 

        if not self.is_sleeping:
            # --- [ä¿®æ”¹] ä½¿ç”¨ personality_system è¨ˆç®—å‡ºçš„å‹•æ…‹åƒæ•¸ ---
            self.emotion_system.decay_emotions(self.personality_system.effective_mood_stability)
            self.emotion_system.decay_core_affect(is_sleeping=False)
            self.emotion_system.apply_random_fluctuations(self.personality_system.effective_mood_stability)
            # --- ä¿®æ”¹çµæŸ ---
            self.perform_daily_news_search_async()
            
        self.memory_system.periodic_maintenance()
        # self.personality_system.periodic_maintenance() # --- [ç§»å‹•] å·²ç§»åˆ°å‰é¢ ---
        return {"new_emotion_for_ui": "sleepy" if self.is_sleeping else self.emotion_system.get_dominant_emotion_for_display()}

    def get_full_debug_status_report(self) -> str:
        """ç”¢ç”Ÿä¸€ä»½åŒ…å«æ‰€æœ‰ä¸»è¦ç‹€æ…‹çš„è©³ç´°å ±å‘Šå­—ä¸²ï¼Œç”¨æ–¼æ—¥èªŒè¨˜éŒ„ã€‚"""
        report_parts = [
            "\n\n--- å®Œæ•´ç‹€æ…‹å ±å‘Š ---",
            f"**ä½¿ç”¨è€… ID:** {self.user_id}",
            f"**ç¡çœ ç‹€æ…‹:** {'æ˜¯' if self.is_sleeping else 'å¦'}",
            f"**ä¸Šæ¬¡äº’å‹•æ™‚é–“:** {datetime.fromtimestamp(self.last_interaction_time).strftime('%Y-%m-%d %H:%M:%S')}",
            "---",
            "**[æ ¸å¿ƒæƒ…æ„Ÿ]**",
            f"  - æ„‰æ‚…åº¦ (Valence): {self.emotion_system.core_affect['valence']:.3f}",
            f"  - å–šé†’åº¦ (Arousal): {self.emotion_system.core_affect['arousal']:.3f}",
            "---",
            "**[ä¸»è¦é›¢æ•£æƒ…ç·’ (Top 5)]**"
        ]
        top_emotions = sorted(self.emotion_system.get_current_emotions().items(), key=lambda item: item[1], reverse=True)[:5]
        for name, value in top_emotions:
            report_parts.append(f"  - {name}: {value:.3f}")
        report_parts.append("---")
        report_parts.append("**[å€‹æ€§ç‰¹è³ª (OCEAN)]**")
        for key, value in self.personality_system.character_traits.items():
            report_parts.append(f"  - {key.replace('ocean_', '').capitalize()}: {value:.3f}")
        report_parts.append(f"  - ä¾æˆ€åº¦ (Attachment): {self.personality_system.attachment_score:.3f}")
        report_parts.append("---")
        report_parts.append("**[è¨˜æ†¶é«”]**")
        report_parts.append(f"  - å°è©±æ­·å²é•·åº¦: {len(self.llm_history)} æ¢")
        report_parts.append(f"  - å€‹é«”ç‰¹å¾µå¿«å–æ•¸é‡: {sum(len(v) for v in self.personality_system.characteristics_cache.values())} æ¢")
        report_parts.append("--- å ±å‘ŠçµæŸ ---\n")
        return "\n".join(report_parts)

    def log_next_llm_prompt(self):
        """å»ºæ§‹ä¸€å€‹æ¸¬è©¦ç”¨çš„LLMæç¤ºä¸¦å°‡å…¶è¨˜éŒ„ä¸‹ä¾†ï¼Œä½†ä¸ç™¼é€ã€‚"""
        logging.info("--- æº–å‚™è¨˜éŒ„ä¸‹æ¬¡ LLM æç¤º (æ¸¬è©¦æ¨¡å¼) ---")
        test_message = "ä½ å¥½å—ï¼Ÿå¯ä»¥è·Ÿæˆ‘èªªèªªé—œæ–¼é«˜é›„çš„è¶£äº‹å—ï¼Ÿ"
        prompt_details = self._build_llm_prompt(test_message, {}, "debug_prompt_test")
        full_prompt_text = "\n".join(part["parts"][0]["text"] for part in prompt_details["contents"])
        logging.debug(f"--- å®Œæ•´çš„ LLM æç¤ºå…§å®¹ ---\n{full_prompt_text}\n--- æç¤ºçµæŸ ---")
        messagebox.showinfo("æç¤ºå·²è¨˜éŒ„", "ä¸€å€‹æ¸¬è©¦ç”¨çš„å®Œæ•´ LLM æç¤ºå·²è¼¸å‡ºåˆ°æ—¥èªŒæª”æ¡ˆä¸­ã€‚")

    def test_search(self):
        """åŸ·è¡Œä¸€æ¬¡æ¸¬è©¦æœå°‹ä¸¦è¨˜éŒ„çµæœã€‚"""
        if self.search and self.search.is_enabled:
            query = "é«˜é›„ä»Šå¤©å¤©æ°£"
            logging.info(f"--- æ‰‹å‹•åŸ·è¡Œæ¸¬è©¦æœå°‹ --- : '{query}'")
            result = self.search.search(query)
            logging.info(f"æœå°‹çµæœ: {result}")
            messagebox.showinfo("æœå°‹æ¸¬è©¦", f"å·²åŸ·è¡Œå° '{query}' çš„æœå°‹ï¼Œè«‹æª¢æŸ¥æ—¥èªŒæŸ¥çœ‹çµæœã€‚")
        else:
            logging.warning("--- æœå°‹æ¸¬è©¦å¤±æ•—ï¼šæœå°‹æœå‹™æœªå•Ÿç”¨ ---")
            messagebox.showwarning("æœå°‹æœªå•Ÿç”¨", "æœå°‹åŠŸèƒ½æœªå•Ÿç”¨æˆ–æœªè¨­å®šé‡‘é‘°/CX IDã€‚")

    def process_direct_user_feedback(self, feedback_type: str, feedback_text: str) -> Optional[Dict[str, Any]]:
        """è™•ç†ä½¿ç”¨è€…å°å¯µç‰©ä¸Šä¸€å¥è©±çš„ç›´æ¥å›é¥‹ï¼ŒåŒ…å« LLM åˆ†æã€‚"""
        if not self.last_pet_spoken_response:
            return {
                "display_text": "å—¯ï¼Ÿæˆ‘å¥½åƒå¿˜è¨˜ä¸Šä¸€å¥èªªäº†ä»€éº¼äº†...",
                "new_emotion_for_ui": self.emotion_system.get_dominant_emotion_for_display(),
                "tag": "system"
            }
        logging.info(f"Processing direct user feedback: Type='{feedback_type}', Text='{feedback_text}'")
        self.memory_system.save_memory(
            content=f"ä½¿ç”¨è€…å°ã€Œ{self.last_pet_spoken_response}ã€çš„å›é¥‹({feedback_type}): {feedback_text}",
            importance=3, pet_emotions=self.emotion_system.get_current_emotions()
        )
        if feedback_type == "positive":
            self.personality_system.handle_significant_event("user_praised_pet", strength_modifier=1.2)
            if feedback_text:
                self.personality_system.add_or_update_characteristic(
                    trait_type=config.TRAIT_TYPE_RESPONSE_STYLE,
                    trait_key=f"user_likes_style_{uuid.uuid4().hex[:6]}",
                    trait_value=f"å–œæ­¡å°æ˜Ÿèªªã€Œ{self.last_pet_spoken_response[:30]}...ã€å› ç‚ºã€Œ{feedback_text[:40]}...ã€",
                    source=config.SOURCE_USER_FEEDBACK_POSITIVE, initial_relevance_base=0.75
                )
            return {"display_text": random.choice(["å¤ªå¥½äº†ï¼ä½ å–œæ­¡çœŸæ˜¯å¤ªæ£’äº†ï¼ğŸ˜„", "å˜¿å˜¿ï¼Œè¬è¬ä½ çš„è‚¯å®šï¼"]), "tag": "pet"}
        elif feedback_type == "negative":
            self.personality_system.handle_significant_event("user_scolded_pet_critical", strength_modifier=1.2)
            return {"display_text": random.choice(["å–”ä¸...æˆ‘æœƒè¨˜ä¸‹ä¾†ï¼Œä¸‹æ¬¡æ”¹é€²çš„ã€‚", "å°ä¸èµ·ï¼Œå¦‚æœè®“ä½ æ„Ÿè¦ºä¸å¥½äº†..."]), "tag": "pet"}
        elif feedback_type == "correction" and self.llm:
            thread = threading.Thread(target=self._learn_from_correction_worker, args=(self.last_pet_spoken_response, self.last_user_input_leading_to_response, feedback_text))
            thread.daemon = True
            thread.start()
            return {"display_text": "å•Šï¼ŒåŸä¾†æ˜¯é€™æ¨£ï¼éå¸¸æ„Ÿè¬ä½ çš„æŒ‡æ­£ï¼Œæˆ‘å­¸åˆ°äº†ï¼ğŸ§ âœ¨", "tag": "pet"}
        return None

    def _learn_from_correction_worker(self, pet_original_response: str, user_original_input: str, user_correction: str):
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­ï¼Œä½¿ç”¨LLMåˆ†æä½¿ç”¨è€…çš„ç³¾æ­£ä¸¦å­¸ç¿’ã€‚"""
        if not self.llm: return
        logging.info("WORKER: Starting learning from user correction.")
        prompt = (
            f"æˆ‘å…ˆå‰å°æå•ã€Œ{user_original_input}ã€çµ¦å‡ºäº†å›æ‡‰ï¼šã€Œ{pet_original_response}ã€ã€‚"
            f"ä½¿ç”¨è€…æŒ‡æ­£èªªï¼šã€Œ{user_correction}ã€ã€‚\n\n"
            "è«‹åˆ†æé€™æ¬¡çš„ä¿®æ­£ï¼Œä¸¦ä»¥JSONæ ¼å¼è¼¸å‡ºï¼š\n"
            "1. `error_summary`: ç°¡è¦ç¸½çµæˆ‘åŸå§‹å›æ‡‰ä¸­å¯èƒ½å‡ºéŒ¯çš„é»ã€‚\n"
            "2. `corrected_fact`: å¾ä¿®æ­£ä¸­æå–å‡ºä¸€å€‹å…·é«”çš„ã€Œæ­£ç¢ºäº‹å¯¦ã€æˆ–ã€Œä½¿ç”¨è€…åå¥½ã€ã€‚\n"
            "3. `learning_point_type`: åˆ¤æ–·å­¸ç¿’é»é¡å‹ï¼š[\"factual_correction\", \"user_preference\"].\n"
            "```json\n"
            "{\n"
            "  \"error_summary\": \"å¤ªé™½æ˜¯è¡Œæ˜Ÿ\",\n"
            "  \"corrected_fact\": \"å¤ªé™½æ˜¯æ†æ˜Ÿ\",\n"
            "  \"learning_point_type\": \"factual_correction\"\n"
            "}\n"
            "```"
        )
        try:
            analysis_result = self.llm.generate_content({"contents": [{"role": "user", "parts": [{"text": prompt}]}], "generation_config": {"temperature": 0.2}})
            parsed_data = self.llm._parse_structured_output(analysis_result.get("spoken_response", ""))
            if parsed_data.get("corrected_fact"):
                self.personality_system.add_or_update_characteristic(
                    trait_type=config.TRAIT_TYPE_USER_INFO,
                    trait_key=f"correction_{parsed_data.get('error_summary', 'unknown')[:15]}",
                    trait_value=parsed_data["corrected_fact"],
                    source=config.SOURCE_USER_FEEDBACK_NEGATIVE, initial_relevance_base=0.88
                )
                logging.info(f"WORKER: Learned from correction: {parsed_data['corrected_fact']}")
        except Exception as e:
            logging.error(f"WORKER: Error during learning from correction: {e}", exc_info=True)

    def initial_personality_setup_async(self):
        """éåŒæ­¥åœ°è§¸ç™¼å•Ÿå‹•æ™‚çš„å€‹æ€§åŒ–æœå°‹ã€‚"""
        if int(self.settings.get(config.SETTING_INITIAL_PERSONALITY_SETUP_DONE, 0)) == 1:
            logging.info("Initial personality setup already done. Skipping.")
            return
        logging.info("Dispatching initial personality setup to worker...")
        thread = threading.Thread(target=self._initial_personality_setup_worker)
        thread.daemon = True
        thread.start()

    def _initial_personality_setup_worker(self):
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­åŸ·è¡Œæœå°‹ä¸¦å­¸ç¿’åˆå§‹çŸ¥è­˜ã€‚"""
        if not self.search or not self.search.is_enabled:
            logging.warning("WORKER: Initial setup skipped, search service is not available.")
            self.db.save_app_setting(config.SETTING_INITIAL_PERSONALITY_SETUP_DONE, 1)
            self.settings[config.SETTING_INITIAL_PERSONALITY_SETUP_DONE] = 1
            return
        
        logging.info("WORKER: Starting initial personality setup via search.")
        user_culture = self.personality_system.demographics.get(config.SETTING_DEMO_CULTURE, "å°ç£")
        queries = [f"{user_culture}æ–‡åŒ–ä¸­çš„æœ‰è¶£ç¿’ä¿—", "é—œæ–¼å¯µç‰©çš„æœ‰è¶£çŸ¥è­˜", "ä¸€å¥éš¨æ©Ÿçš„å‹µå¿—åè¨€"]
        
        learned_facts_count = 0
        for query in queries:
            search_response = self.search.search(query, num_results=1)
            time.sleep(1)
            if search_response and search_response.get("results"):
                result_text = search_response["results"][0]
                snippet_match = re.search(r"æ‘˜è¦:\s*(.*)", result_text)
                fact_to_learn = snippet_match.group(1).strip() if snippet_match else result_text.split("\n")[0]
                if fact_to_learn:
                    self.personality_system.add_or_update_characteristic(
                        trait_type=config.TRAIT_TYPE_PET_SELF_CONCEPT,
                        trait_key=f"initial_setup_{query[:10]}",
                        trait_value=fact_to_learn[:200],
                        source=config.SOURCE_SYSTEM_INITIATED, initial_relevance_base=0.5
                    )
                    learned_facts_count += 1
        
        self.db.save_app_setting(config.SETTING_INITIAL_PERSONALITY_SETUP_DONE, 1)
        self.settings[config.SETTING_INITIAL_PERSONALITY_SETUP_DONE] = 1
        logging.info(f"WORKER: Initial personality setup finished. Learned {learned_facts_count} facts.")

    def perform_daily_news_search_async(self):
        """éåŒæ­¥åœ°è§¸ç™¼æ¯æ—¥æ–°èæœå°‹ã€‚"""
        if not int(self.settings.get(config.SETTING_SEARCH_DAILY_NEWS_ENABLED, 0)): return
        now_ts = time.time()
        last_search_ts = float(self.settings.get(config.SETTING_LAST_NEWS_SEARCH_TIMESTAMP, 0))
        if (now_ts - last_search_ts) < (23 * 3600): return
        
        logging.info("Dispatching daily news search to worker...")
        thread = threading.Thread(target=self._perform_daily_news_search_worker)
        thread.daemon = True
        thread.start()

    def _perform_daily_news_search_worker(self):
        """åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­åŸ·è¡Œæ¯æ—¥æ–°èæœå°‹ä¸¦å„²å­˜æ‘˜è¦ã€‚"""
        if not self.search or not self.search.is_enabled:
            logging.warning("WORKER: Daily news search skipped, search service is not available.")
            return

        logging.info("WORKER: Starting daily news search.")
        query = "ä»Šæ—¥åœ‹éš›ç„¦é»æ–°è"
        search_response = self.search.search(query, num_results=2)
        if search_response and search_response.get("results"):
            news_summary = "æˆ‘ä»Šå¤©ç€è¦½åˆ°ä¸€äº›è³‡è¨Šæ‘˜è¦ï¼š\n" + "\n".join(search_response["results"])
            self.personality_system.add_or_update_characteristic(
                trait_type=config.TRAIT_TYPE_KEY_MEMORY_SUMMARY,
                trait_key="daily_news_summary_for_pet",
                trait_value=news_summary[:800],
                source=config.SOURCE_SYSTEM_INITIATED, initial_relevance_base=0.75
            )
            now_ts = time.time()
            self.db.save_app_setting(config.SETTING_LAST_NEWS_SEARCH_TIMESTAMP, now_ts)
            self.settings[config.SETTING_LAST_NEWS_SEARCH_TIMESTAMP] = now_ts
            logging.info("WORKER: Daily news summary stored.")
    # è«‹å°‡é€™å€‹æ–¹æ³•åŠ å…¥åˆ° core/pet_logic.py çš„ PetLogic é¡åˆ¥ä¸­

    def check_task_reminders(self) -> Optional[Dict[str, Any]]:
        """æª¢æŸ¥æ˜¯å¦æœ‰éœ€è¦æé†’çš„ä»»å‹™ï¼Œä¸¦ä½¿ç”¨ LLM ç”Ÿæˆæé†’èªå¥ã€‚"""
        if self.is_processing_llm or getattr(self, "is_generating_proactive_chat", False):
            return None
            
        active_tasks = self.db.get_tasks(self.user_id, include_completed=False)
        if not active_tasks:
            return None

        now_dt = datetime.now()
        now_ts = now_dt.timestamp()

        for task in active_tasks:
            task_id = task['id']
            last_reminded_ts = self.logic.current_task_reminders.get(task_id, 0)
            
            should_remind_now = False
            reminder_urgency = "normal"
            reminder_interval_hours = 24 # é è¨­æ¯å¤©æé†’

            if task['due_at']:
                due_dt = datetime.fromtimestamp(task['due_at'])
                time_to_due_sec = (due_dt - now_dt).total_seconds()

                if time_to_due_sec < 0:
                    reminder_urgency = "overdue"
                    reminder_interval_hours = random.uniform(2, 4)
                elif time_to_due_sec < 3600 * 2:
                    reminder_urgency = "soon"
                    reminder_interval_hours = random.uniform(0.5, 1.5)
                elif time_to_due_sec < 3600 * 24:
                    reminder_urgency = "normal"
                    reminder_interval_hours = random.uniform(3, 6)
                else:
                    reminder_urgency = "gentle"
                    reminder_interval_hours = random.uniform(20, 36)
            else: # æ²’æœ‰æˆªæ­¢æ—¥æœŸçš„ä»»å‹™
                if (now_ts - task['created_at']) > 3600 * 24 * 2: # å…©å¤©ä»¥ä¸Šçš„èˆŠä»»å‹™æ‰æé†’
                     reminder_interval_hours = random.uniform(48, 72)
                else:
                    continue

            if (now_ts - last_reminded_ts) > (reminder_interval_hours * 3600):
                should_remind_now = True
            
            if should_remind_now and reminder_urgency not in ["soon", "overdue"] and random.random() < 0.3:
                should_remind_now = False

            if should_remind_now:
                logging.info(f"Task reminder triggered for: '{task['description']}' (Urgency: {reminder_urgency})")
                
                due_date_str = f"æœŸé™ï¼š{datetime.fromtimestamp(task['due_at']).strftime('%Y-%m-%d %H:%M')}" if task['due_at'] else "ç„¡ç‰¹å®šæœŸé™"
                reminder_theme = (f"ä½ éœ€è¦ç”¨å‹å–„ä¸”ç¬¦åˆä½ å€‹æ€§çš„æ–¹å¼ï¼Œæé†’ä½¿ç”¨è€…é—œæ–¼ä¸€å€‹ä»»å‹™ã€‚ä»»å‹™æè¿°ï¼šã€Œ{task['description']}ã€ã€‚{due_date_str}ã€‚"
                                  f"æé†’çš„ç·Šæ€¥ç¨‹åº¦æ˜¯ã€Œ{reminder_urgency}ã€ã€‚")

                # æ¨™è¨˜æ­£åœ¨è™•ç†ä¸­ï¼Œé˜²æ­¢å…¶ä»–ä¸»å‹•è¡Œç‚ºè§¸ç™¼
                self.is_processing_llm = True 
                
                try:
                    prompt_details = self._build_llm_prompt(reminder_theme, {}, "task_reminder_phrasing")
                    llm_result = self.llm.generate_content(prompt_details)
                    spoken_response = llm_result.get("spoken_response")
                    if spoken_response:
                        self.logic.current_task_reminders[task_id] = now_ts
                        return {
                            "display_text": spoken_response,
                            "new_emotion_for_ui": self.emotion_system.get_dominant_emotion_for_display(),
                            "tag": "task_reminder"
                        }
                finally:
                    self.is_processing_llm = False
                
                break # ä¸€æ¬¡åªæé†’ä¸€å€‹ä»»å‹™
        
        return None
